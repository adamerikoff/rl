# Reinforcement Learning Models and Algorithms (Chronological Order)

| Year   | Algorithm/Model                                          | Description                                                                 |
|--------|----------------------------------------------------------|-----------------------------------------------------------------------------|
| 1951   | Turing's Chess-Playing Algorithm                         | Conceptual precursor to RL.                                                |
| 1956   | Samuel’s Checkers Program                                | Early machine learning using heuristics.                                   |
| 1959   | Widrow-Hoff Learning Rule                                | Inspired early neural network training.                                    |
| 1960s  | Dynamic Programming                                      | Foundational method by Richard Bellman.                                    |
| 1983   | Temporal Difference (TD) Learning                        | Introduced by Sutton, core of RL.                                          |
| 1989   | Q-Learning                                               | Off-policy value-based RL by Watkins.                                      |
| 1992   | SARSA (State-Action-Reward-State-Action)                 | On-policy learning algorithm.                                              |
| 1994   | Actor-Critic Methods                                     | Hybrid value-policy learning.                                              |
| 1996   | Function Approximation in RL                             | Neural networks applied to RL.                                             |
| 1997   | Monte Carlo Methods in RL                                | Episodic learning algorithms.                                              |
| 1999   | Policy Gradient Methods                                  | Foundation for modern policy-based RL.                                     |
| 2000   | Options Framework                                        | Hierarchical RL for temporal abstraction.                                  |
| 2001   | Least-Squares Policy Iteration (LSPI)                   | Efficient linear approximations.                                           |
| 2003   | Fitted Q-Iteration                                       | Extension of value-based learning.                                         |
| 2005   | Bayesian RL                                              | Probabilistic modeling for exploration-exploitation.                       |
| 2013   | Deep Q-Networks (DQN)                                    | Landmark deep RL method by Mnih et al.                                     |
| 2014   | Deterministic Policy Gradient (DPG)                     | Introduced by Silver et al.                                                |
| 2015   | Deep Deterministic Policy Gradient (DDPG)               | Improvement on DPG by Lillicrap et al.                                     |
| 2015   | Double Q-Learning                                        | Mitigates overestimation bias in Q-learning.                               |
| 2015   | Prioritized Experience Replay (PER)                     | Efficient replay sampling.                                                 |
| 2016   | Trust Region Policy Optimization (TRPO)                 | Optimization algorithm by Schulman et al.                                  |
| 2016   | Generalized Advantage Estimation (GAE)                  | Improves advantage estimation.                                             |
| 2016   | Asynchronous Advantage Actor-Critic (A3C)               | Parallelized actor-critic learning by Mnih et al.                          |
| 2016   | AlphaGo                                                 | Go-playing AI by Silver et al.                                             |
| 2016   | Guided Policy Search (GPS)                              | Supervised RL by Levine et al.                                             |
| 2017   | Proximal Policy Optimization (PPO)                      | Improved TRPO by Schulman et al.                                           |
| 2017   | Advantage Actor-Critic (A2C)                            | Synchronous version of A3C.                                                |
| 2017   | Actor-Critic using Kronecker-Factored Trust Region (ACKTR) | Efficient trust region method.                                            |
| 2017   | Rainbow DQN                                             | Combines multiple DQN improvements.                                        |
| 2017   | AlphaZero                                               | Chess and Go AI by Silver et al.                                           |
| 2017   | Hierarchical Reinforcement Learning (HRL)               | Structured decision-making approach.                                       |
| 2018   | Soft Actor-Critic (SAC)                                 | Entropy-regularized RL algorithm.                                          |
| 2018   | Evolution Strategies (ES)                               | Alternative optimization method.                                           |
| 2018   | Curiosity-Driven Exploration                            | Intrinsic rewards for exploration.                                         |
| 2018   | Multi-Agent Deep Deterministic Policy Gradient (MADDPG) | Multi-agent learning algorithm.                                            |
| 2019   | AlphaStar                                               | StarCraft II AI by Vinyals et al.                                          |
| 2019   | Five (Dota-2-playing bots)                              | OpenAI’s AI team for Dota 2.                                               |
| 2019   | Dreamer                                                 | Model-based RL for world model learning.                                   |
| 2019   | Model-Agnostic Meta-Learning (MAML)                     | Meta-learning framework.                                                   |
| 2019   | Exploration via Random Network Distillation (RND)       | Encourages exploration through novelty.                                    |
| 2020   | MuZero                                                  | Combines model-based and model-free methods.                               |
| 2020   | Recurrent Experience Replay in RL (R2D2)                | Robust memory learning for RL.                                             |
| 2020   | Agent57                                                 | General-purpose RL agent.                                                  |
| 2020   | Deep Ensemble Q-Learning (DEQL)                         | Ensemble methods for Q-learning.                                           |
| 2021   | Go-Explore                                              | Efficient exploration in sparse rewards.                                   |
| 2021   | Reward-Free RL                                          | Generalization to new tasks.                                               |
| 2022   | Offline RL (Batch RL)                                   | Learning from fixed datasets.                                              |
| 2023   | Implicit Q-Learning (IQL)                               | Improved offline RL performance.                                           |

---

### **Broader Techniques**
| Technique                          | Description                                                               |
|------------------------------------|---------------------------------------------------------------------------|
| Inverse Reinforcement Learning (IRL) | Learning reward functions from behavior.                                 |
| Imitation Learning (IL)            | Supervised learning of policies.                                          |
| Hierarchical RL (HRL)              | Multi-level decision-making structures.                                   |
| Multi-Agent RL (MARL)              | Learning in cooperative/competitive environments.                         |
| Meta-RL                            | Generalizing across multiple tasks or environments.                       |
| Self-Supervised RL                 | Learning representations without external rewards.                        |
